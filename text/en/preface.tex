\pagestyle{plain}
\setcounter{page}{1}

\chapter{Introduction}

In this thesis, we exploring the possibility of speeding  up cluster analysis by parallelization and dependency between input data properties and different parallelization approaches.\\
The main motivations for choosing this topic was a great growth in demand for processing large data. These data are collected from many sources such as social networks, financial markets, medical observation, space exploration or other scientific data. Even if this data seem like different, they have something in common. The first thing is that they contain may useful informations for scientists, sociologist, medics but even for business analysts, brokers or for advertisement. The second thing is that they must be processed by computers because of data size and compute time intensity. \\

Problem is that the data grows much faster than compute performance of the most modern hardware so neither computers have enough processing power for processing this data if we use standard algorithms. For this purposes there are known technologies for processing or simplifying big data including cluster analysis, generic algorithms, machine learning or simulation.
Problem is that in modern big data world neither of these techniques in conjunction with traditional hardware are enough so we are enforced to use other possibilities. \\
Because processing single data is not so complex and processing them is very often independent, ideal hardware should contains many of simple and cheap compute units capable of operating in parallel on huge amount of data. This is exactly way followed by manufacturers of the most modern hardware, which is not designed for the highest performance of individual cores but for the best parallel work reached by increasing number of cores on single chips and speeding up communication between cores. \\

As a reasonable solution is to use compute performance of graphics cards which contains thousands of simple cores. Even though their purpose is primarily graphical tasks, cards allows to use theirs power to common computations (general-purpose computing on graphics processing units - GPGPU). Their performance with each generation increases rapidly so they are great opportunity for making current compute-intensive algorithmic problems efficient and less time-consuming and make them usable for areas in which their application was inefficient or even not possible.\\
Because graphical cards was originally specialized for computation of graphical primitives, which contains many of separate and independent tasks so the problem is when we need to process tasks which are not so similar to graphical primitives. This is usually the problem of processing big data in some more sophisticated way and we must solve the problem with dependencies between single computations. \\

As a representative of such problems we chose cluster analysis, especially k-means algorithm - algorithm, which is looking for clusters in the input data and finds a representative (called mean or centroid) of each cluster. It works iteratively and in each step, it finds nearest representative for each object and than compute new representative for each cluster. This algorithm has a very broad scope of application and it is very time consuming for big data so it is really useful to accelerate it using the most modern hardware.\\

Problem is that this algorithm contains several compute and data dependencies which must be solved. We could easily compute the nearest representative for each object but problem is when we want to compute update representative from object assigned to same cluster. Because we have large number of objects and so large number of compute cores, we want to parallelize the computation in object manner - each object is processed by single core. Problem is that we need to accumulate data for each of the newest representative and problem is, that it could be accessed by many cores at a time. We must choose some synchronization primitive, which slows down the computation, or choose a different way of parallelizing this task such as parallelizing computation in representative manner - each new representative is processed by single compute core and it iterates through all objects which could be slow too.\\

There are also problems with data size and memory usage efficiency. GPU has several types of memory. They differ significantly in size and latency - bigger memory is usually slower with bigger latency and vice versa, smaller memory is faster with small latency times. The main goal is to use the memory as effectively as possible. Problem is that the big input data are sometimes much larger than the biggest GPU memory so we must design a way, how to swap data between host and GPU and between GPU's memory effectively. This is problem because of variety of input data so we must try several versions and find out which approach is the best with respect to the different input data.\\

We must solve this by modification of the k-means algorithm to GPU environment and develop several versions to find out which approach fits the best for concrete type of data.