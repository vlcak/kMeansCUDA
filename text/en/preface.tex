\pagestyle{plain}
\setcounter{page}{1}

\chapter{Introduction}
The main motivations for choosing this topic was a great growth in demands on machine computing. Especially, demands on computation time and amount of work are a challenge, because there are needs for in-time processing of huge volumes of data. With utilization of modern chips supporting parallel computations, this problem is very often solved by splitting basic algorithms into independent tasks and computing them concurrently. Problem is that the decomposition of algorithms into parallel tasks is usually hard and needs further research.\\

In this thesis, we are exploring the possibilities of improving the cluster analysis by effective implementation of its algorithm on parallel architectures like multicore CPUs and GPUs. Mainly, we are investigating different parallelization approaches and their dependence on the input data properties in centroid based clustering.
The data are collected from many sources such as social networks, financial markets, medical observation, space exploration and many others. Even if the data comes from different sources, they have something in common. They contain useful informations for scientists, sociologist, medics, but even for business analysts, brokers, or for advertisement.
If we are able to parallelize the cluster analysis, we can solve problems with its runtime performance, which is a concern as data are growing rapidly. Our solution could make cluster analysis usable in areas where basic algorithms was inappropriate due to specific demands on computation performance and/or data size.\\

%, especially when finding the correct parameter K can only be done by performing several runs with different numbers of clusters and initial seeds~\cite{Zechner09}.\\
%Clustering data from the mentioned sources has a challenge - data are simply too large and processing them could take an unbearable amount of time. This problem could be solved by parallelization, but especially on common hardware, it may not be enough. 

If we focus on the parallelization of the centroid based clustering, we must pick a particular algorithm for the analysis. As a representative, we chose the k-means algorithm, because it is widely used for its low complexity and well understood mathematical properties. It is also one of the most efficient clustering algorithms proposed in the literature of data clustering~\cite{Aggarwal13}.\\

The k-means clustering could be defined as follows: a set of points and a number of clusters $k$ are given. We need to group points into non-overlapping clusters with special point called centroid. This cluster is composed as a mean of all points for which cluster centroid is the nearest centroid using appropriate metric, usually the Euclidean distance~\cite{Zechner09}. The optimal set of centroids can be found by by minimizing the sum of distances between points and a mean of the enclosing cluster represented by the closest centroid. Solving this optimization problem was proven to be NP-hard even for two clusters~\cite{Drineas04}, however, k-means approximate the optimal solution by converging to a local minimum of previously described minimization problem depending on the initial centroids~\cite{Bottou95}.\\

K-means algorithm starts by choosing K points as a initial centroids. Usually, random points from the input set are used. This phase is known as the seeding stage.
Then, in the labeling stage, each point is assigned to the nearest centroid.
When clusters are formed, the centroid for each cluster is updated.
Algorithm works iteratively by repeating the labeling and the update stage until centroids does not change between two iterations or after a given number of iterations.\\

Because the k-means algorithm is composed of many data parallel tasks, we should use hardware, which could take advantage of this attribute.
One of the best candidates for the parallelization of k-means are graphics cards~\cite{Zechner09} as a low-cost, highly parallel streaming processor. They contains thousands of simple cores capable of general math operations. Even though their purpose is primarily computations of graphical tasks, these cards also allow to use theirs computing performance for general purpose computations (GPGPU).
If we use potential of GPU for k-means algorithm, we can use the data parallelism in k-means algorithm and divide the work among all cores and make the computation much faster. For example, we can compute the nearest centroid for each point in parallel and utilize all available cores.\\
 
%   Their performance with each generation increases rapidly so they are a great opportunity for making current compute-intensive algorithmic problems efficient and less time-consuming.\\

Parallelization of k-means algorithm on GPU brings also some challenges. We must optimize the algorithm to massively parallel environment. This means that the work must be divided between thousands of computing cores and we must take care of their effective load balancing, because we could loose a lot of performance when only part of all available cores are utilized. Also the memory usage must be designed to fit the GPU memory model because we will loose many time by waiting for memory access to slower memories.\\

%If we omit general parallelization issues such as synchronization and non-parallelizable parts of algorithm, we must also challenge new problems arising from a different parallelization environment. 

The conversion of k-means algorithm to GPU was already introduced in 2009 by Hong-Tao Bai et al.~\cite{Hong09}. However, their work does not explore detailed possibilities of parallelization.
In this thesis, we are analyzing the input data properties and their impact on the computing performance at different approaches to the k-means parallelization. This is very crucial for this algorithm, which execution is highly affected by the input data that could vary greatly.\\

Also, because of a rapid development of GPGPU technologies and GPUs itself, we want to explore new GPU possibilities provided by the current hardware.\\

% TODO - k-means specific properties - input types

%Problem is that the data amount grows sometimes even faster than compute performance of the most modern hardware so neither computers have enough processing power for processing this data when we use standard algorithms. For this purposes there are known technologies for processing or simplifying big data including cluster analysis, generic algorithms, machine learning or simulation.
%Problem is that in modern big data world neither of these techniques in conjunction with traditional hardware are enough so we are enforced to use other possibilities. \\
%Because processing single item is not so complex and processing them is very often independent, ideal hardware should contain many of simple and cheap compute units capable of parallel operations on huge amount of data. Luckily, this is the way followed by manufacturers of the most modern hardware, which is not designed only for the highest performance of individual cores but also for the best parallel cooperative work reached by increasing the number of cores on single chip and speeding up the communication between cores. \\

%A reasonable solution is to use compute performance of graphics cards which contains thousands of simple cores. Even though their purpose is primarily graphical tasks, cards allows to use theirs power to common computations (general-purpose computing on graphics processing units - GPGPU). Their performance with each generation increases rapidly so they are great opportunity for making current compute-intensive algorithmic problems efficient and less time-consuming and make them usable for areas in which their application was inefficient or even not possible.\\
%Because graphical cards was originally specialized for computation of graphical primitives, whose contain many of separate and independent tasks, the problem is when we need to process tasks that are not so similar to graphical primitives. This is usually the problem of processing big data in some more sophisticated way and we must solve the problem by adding additional dependencies between computations. \\



%Parallelizing this algorithm contains several problems. The first thing is that we must solve race conditions, for example, when new centroids are computed from newly assigned objects. The new value of centroid is computed as a mean from all object assigned to same centroid so we need to accumulate all these objects in single variable from different threads and this could be the problem.\\

%Problem is that this algorithm contains several compute and data dependencies which must be solved. We could easily compute the nearest representative for each object but problem is when we want new representatives for the next step based on newly assigned objects. Because we have a large number of objects and a large number of compute cores, we want to parallelize the computation so each object is processed by single core. Problem is that we need to accumulate data for each of emerging representatives and the problem is, that it could be accessed by many cores at a time. We must choose some synchronization primitive which slows down the computation, or choose a different way of parallelizing this task such as parallelizing the computation in representative manner - each new representative is processed by single compute core and it iterates through all objects which could be slow too, because we could not have so many clusters as compute cores.\\

%There are also problems with data size and memory usage efficiency. GPU has several types of memory. They differ significantly in size and latency - bigger memory is usually slower with bigger latency and vice versa, smaller memory is faster with small latency times. The main goal is to use the memory as effectively as possible. Problem is that the big input data are sometimes much larger than the biggest GPU memory so we must design a way, how to swap data between host and GPU and between GPU's memory effectively. This is problem because of variety of input data so we must try several versions and find out which approach is the best with respect to the different input data.\\

%We must solve this by modification of the k-means algorithm to GPU environment and develop several versions to find out which approach fits the best for concrete type of data.

%contributions
%We will focus on different approaches to GPGPU parallelization of k-means algorithm and impacts of different data types on performance. We tried to utilize the data properties for designing different algorithm versions and investigated which version performs better on specific data by comparing versions on all data types.\\
%outline
The \hyperref[sec:clusteranalysis]{Chapter~\ref*{sec:clusteranalysis}} introduce cluster analysis, several types of cluster models and clustering algorithms. The \hyperref[sec:gpgpu]{Chapter~\ref*{sec:gpgpu}} concerns GPGPU, CUDA framework and its analysis. The \hyperref[sec:implementation]{Chapter~\ref*{sec:implementation}} consists of the description of our implementations and used parallelization methods and in the \hyperref[sec:results]{Chapter~\ref*{sec:results}}, we will discuss the results of this thesis and efficiency of k-means parallelization. 