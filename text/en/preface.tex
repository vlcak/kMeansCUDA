\pagestyle{plain}
\setcounter{page}{1}

\chapter{Introduction}

In this thesis, we are exploring the possibilities of improving the cluster analysis by parallelization the cluster analysis algorithm. Mainly, we are investigating different parallelization approaches and their dependence on input data properties.\\
The main motivations for choosing this topic was a great growth in demand for processing large data. The data are collected from many sources such as social networks, financial markets, medical observation, space exploration, or other scientific data. Even if these data seem like different, they have something in common. The first thing is that they contain useful informations for scientists, sociologist, medics, but even for business analysts, brokers or for advertisement. %The second thing is that the data must be processed by computers because of the size and compute time intensity.
\\

As a representative of data processing, we chose Cluster Analysis, especially k-means algorithm~\cite{Aggarwal13, EstivillCastro02}. The main goal of this algorithm is to find representatives of clusters called means or centroids and than split the data into Voronoi cells~\cite{Kim14} based on found means. It works iteratively and in each step, it finds nearest mean for each object and than compute new mean for each cluster based on objects contained in cluster. This algorithm has a very broad scope of application such as machine learning and data mining. K-means algorithm is NP-hard so it could be very time consuming for larger data hence it is really useful to accelerate it using the most modern hardware.\\

Clustering data from the mentioned sources like social networks or science has an challenge - data are simply too large and processing them could take unbearable amount of time. This problem could be solved by parallelization, but especially on common hardware, it may not be enough. \\
Because processing single item in k-means algorithm is not so complex and the computation is independent very often, ideal hardware should contain many simple and cheap compute units capable of parallel operations on huge amount of data. This demand is similar to graphical tasks so utilization of hardware specialized for graphical computations is obvious.\\
Graphics cards which contains thousands of simple cores capable of basic math operations. Even though their purpose is primarily graphical tasks, cards allows to use theirs power to common computations (general-purpose computing on graphics processing units - GPGPU). Their performance with each generation increases rapidly so they are great opportunity for making current compute-intensive algorithmic problems efficient and less time-consuming and make them usable for areas in which their application was inefficient or even not possible.\\
But there is also few problems to be solved arising from transformation of this problem to graphical cards. For example, the size of data could be problem. Special cards have only limited memory resources so the data could not fit there.\\

The conversion of k-means algorithm to GPU was already introduced~\cite{Hong09}, but because of rapid development of \hyphenation{GPGPU} technologies and GPUs itself, we want to explore this possibility on the current hardware. In this thesis, we focused on investigation possibilities of effective utilization contemporary GPUs for the k-means algorithm. This means that we need to take advantage of all hardware and software opportunities provided by GPUs like thousands of cores, different memory types, new synchronization functions and more. This task is very challenging, because k-means algorithm has a highly variable properties based on processed data. If we want exploit as many computing cores as possible, we must take into data characteristics and analyze different approaches to parallelization. Also efficient use of different memory layers brings great benefits, but it is very complex and should be explored together with core usage.\\

% TODO - k-means specific properties - input types

%Problem is that the data amount grows sometimes even faster than compute performance of the most modern hardware so neither computers have enough processing power for processing this data when we use standard algorithms. For this purposes there are known technologies for processing or simplifying big data including cluster analysis, generic algorithms, machine learning or simulation.
%Problem is that in modern big data world neither of these techniques in conjunction with traditional hardware are enough so we are enforced to use other possibilities. \\
%Because processing single item is not so complex and processing them is very often independent, ideal hardware should contain many of simple and cheap compute units capable of parallel operations on huge amount of data. Luckily, this is the way followed by manufacturers of the most modern hardware, which is not designed only for the highest performance of individual cores but also for the best parallel cooperative work reached by increasing the number of cores on single chip and speeding up the communication between cores. \\

%A reasonable solution is to use compute performance of graphics cards which contains thousands of simple cores. Even though their purpose is primarily graphical tasks, cards allows to use theirs power to common computations (general-purpose computing on graphics processing units - GPGPU). Their performance with each generation increases rapidly so they are great opportunity for making current compute-intensive algorithmic problems efficient and less time-consuming and make them usable for areas in which their application was inefficient or even not possible.\\
%Because graphical cards was originally specialized for computation of graphical primitives, whose contain many of separate and independent tasks, the problem is when we need to process tasks that are not so similar to graphical primitives. This is usually the problem of processing big data in some more sophisticated way and we must solve the problem by adding additional dependencies between computations. \\



%Parallelizing this algorithm contains several problems. The first thing is that we must solve race conditions, for example, when new centroids are computed from newly assigned objects. The new value of centroid is computed as a mean from all object assigned to same centroid so we need to accumulate all these objects in single variable from different threads and this could be the problem.\\

%Problem is that this algorithm contains several compute and data dependencies which must be solved. We could easily compute the nearest representative for each object but problem is when we want new representatives for the next step based on newly assigned objects. Because we have a large number of objects and a large number of compute cores, we want to parallelize the computation so each object is processed by single core. Problem is that we need to accumulate data for each of emerging representatives and the problem is, that it could be accessed by many cores at a time. We must choose some synchronization primitive which slows down the computation, or choose a different way of parallelizing this task such as parallelizing the computation in representative manner - each new representative is processed by single compute core and it iterates through all objects which could be slow too, because we could not have so many clusters as compute cores.\\

%There are also problems with data size and memory usage efficiency. GPU has several types of memory. They differ significantly in size and latency - bigger memory is usually slower with bigger latency and vice versa, smaller memory is faster with small latency times. The main goal is to use the memory as effectively as possible. Problem is that the big input data are sometimes much larger than the biggest GPU memory so we must design a way, how to swap data between host and GPU and between GPU's memory effectively. This is problem because of variety of input data so we must try several versions and find out which approach is the best with respect to the different input data.\\

%We must solve this by modification of the k-means algorithm to GPU environment and develop several versions to find out which approach fits the best for concrete type of data.

%contributions
%We will focus on different approaches to GPGPU parallelization of k-means algorithm and impacts of different data types on performance. We tried to utilize the data properties for designing different algorithm versions and investigated which version performs better on specific data by comparing versions on all data types.\\
%outline
The first chapter introduce reader to cluster analysis, several types of cluster models and clustering algorithms. The next chapter concerns GPGPU, different frameworks and the analysis of CUDA framework. The third chapter consists of description of our implementations and used parallelization methods and in last chapter we will discuss the results of this thesis and effectiveness of k-means parallelization. 