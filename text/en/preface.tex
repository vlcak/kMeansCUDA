\pagestyle{plain}
\setcounter{page}{1}

\chapter{Introduction}

In this thesis, we are exploring the possibilities of improving the cluster analysis by parallelization the cluster analysis algorithm. Mainly, we are investigating different parallelization approaches and their dependence on input data properties.\\
The main motivations for choosing this topic was a great growth in demand for processing large data. The data are collected from many sources such as social networks, financial markets, medical observation, space exploration, or other scientific data. Even if these data seem like different, they have something in common. The first thing is that they contain useful informations for scientists, sociologist, medics, but even for business analysts, brokers or for advertisement. %The second thing is that the data must be processed by computers because of the size and compute time intensity.
\\

Problem is that the data amount is usually too large for processing them on classic computer

Problem is that the data amount grows sometimes even faster than compute performance of the most modern hardware so neither computers have enough processing power for processing this data when we use standard algorithms. For this purposes there are known technologies for processing or simplifying big data including cluster analysis, generic algorithms, machine learning or simulation.
Problem is that in modern big data world neither of these techniques in conjunction with traditional hardware are enough so we are enforced to use other possibilities. \\
Because processing single item is not so complex and processing them is very often independent, ideal hardware should contain many of simple and cheap compute units capable of parallel operations on huge amount of data. Luckily, this is the way followed by manufacturers of the most modern hardware, which is not designed only for the highest performance of individual cores but also for the best parallel cooperative work reached by increasing the number of cores on single chip and speeding up the communication between cores. \\

A reasonable solution is to use compute performance of graphics cards which contains thousands of simple cores. Even though their purpose is primarily graphical tasks, cards allows to use theirs power to common computations (general-purpose computing on graphics processing units - GPGPU). Their performance with each generation increases rapidly so they are great opportunity for making current compute-intensive algorithmic problems efficient and less time-consuming and make them usable for areas in which their application was inefficient or even not possible.\\
Because graphical cards was originally specialized for computation of graphical primitives, whose contain many of separate and independent tasks, the problem is when we need to process tasks that are not so similar to graphical primitives. This is usually the problem of processing big data in some more sophisticated way and we must solve the problem by adding additional dependencies between computations. \\

As a representative of such problems we chose cluster analysis, especially k-means algorithm~\cite{Aggarwal13, EstivillCastro02} - algorithm, which is looking for clusters in the input data, computing representative called mean or centroid for each of found clusters and than split input data into Voronoi cells~\cite{Kim14} based on found representatives representatives. It works iteratively and in each step, it finds nearest representative for each object and than compute new representative for each cluster. This algorithm has a very broad scope of application and it is very time consuming even for smaller data so it is really useful to accelerate it using the most modern hardware for big data.\\

Parallelizing this algorithm contains several problems. The first thing is that we must solve race conditions, for example, when new centroids are computed from newly assigned objects. The new value of centroid is computed as a mean from all object assigned to same centroid so we need to accumulate all these objects in single variable from different threads and this could be the problem.\\
Another problem is the limitation of device memory, because the input data could be simply to big to fit into device memory. We must cycle them on device in some way to solve this problem.
\\
%Problem is that this algorithm contains several compute and data dependencies which must be solved. We could easily compute the nearest representative for each object but problem is when we want new representatives for the next step based on newly assigned objects. Because we have a large number of objects and a large number of compute cores, we want to parallelize the computation so each object is processed by single core. Problem is that we need to accumulate data for each of emerging representatives and the problem is, that it could be accessed by many cores at a time. We must choose some synchronization primitive which slows down the computation, or choose a different way of parallelizing this task such as parallelizing the computation in representative manner - each new representative is processed by single compute core and it iterates through all objects which could be slow too, because we could not have so many clusters as compute cores.\\

%There are also problems with data size and memory usage efficiency. GPU has several types of memory. They differ significantly in size and latency - bigger memory is usually slower with bigger latency and vice versa, smaller memory is faster with small latency times. The main goal is to use the memory as effectively as possible. Problem is that the big input data are sometimes much larger than the biggest GPU memory so we must design a way, how to swap data between host and GPU and between GPU's memory effectively. This is problem because of variety of input data so we must try several versions and find out which approach is the best with respect to the different input data.\\

%We must solve this by modification of the k-means algorithm to GPU environment and develop several versions to find out which approach fits the best for concrete type of data.

%contributions
We will focus on different approaches to massive parallelization k-means algorithm on GPU and impacts of different data types on performance. We tried to utilize the data properties in designing different algorithm versions and investigated which version performs better on concrete data by measuring the speed up of each versions on all different data types.\\
\\
%outline
In the first chapter of this thesis, there is introduction to cluster analysis, several types of cluster models and clustering algorithms. In the second chapter, we will acquaint the reader with GPGPU, different frameworks and we will analyze CUDA framework. In the third chapter we will describe our implementations and used parallelization methods and in last chapter we will discuss the results of this thesis and effectiveness of k-means parallelization. 